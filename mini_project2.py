# -*- coding: utf-8 -*-
"""Mini-Project2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17qwp4qSIegB4_mIAAEauxTgkH5QK1pEF
"""

#Import libraries
import tweepy
import json
import yaml
import pandas as pd

#Connecting to google drive
from google.colab import drive
drive.mount('/content/drive')

#Twitter tokens
doc = '/content/drive/MyDrive/IST652_Spring22/Lecture Week 12/twitter_config.yaml'

with open(doc) as f:
    config = yaml.safe_load(f)

#Setting up Twitter API
auth = tweepy.OAuthHandler(config['CONSUMER_KEY'],config['CONSUMER_SECRET'])
auth.set_access_token(config['OAUTH_TOKEN'],config['OAUTH_SECRET'])
api = tweepy.API(auth)

#Going to work with BBC News
BBC_timeline = api.user_timeline('BBCWorld', count=400)
type(BBC_timeline)

#Converting it into json
BBCWorld_list=list(BBC_timeline)
BBC_json=[tweet._json for tweet in BBC_timeline]
BBC_json

#Creating a dataframe, which would also make it easier to work with
BBC_df = pd.DataFrame(BBC_json)
BBC_df.head()

#Question 1
#Looking at what are the most popular words used in BBC World News and what is its frequency
import nltk
nltk.download('punkt')
BBC_l = [doc['text'] for doc in BBC_json if 'text' in doc.keys()]
BBC_tokens = [tok for tweet in BBC_l for tok in nltk.word_tokenize(tweet)]
BBC_tokens

#Removing the punctuation
#This allows to take in a word, and return True only if it has no letters in it
import re
def alpha_filter(w):
    pattern = re.compile('^[^a-z]+$')
    if (pattern.match(w)):
        return True
    else:
        return False

#Removes the punctuation as the most popular word from out list
BBC_list = [tok for tok in all_stopped_tokens if not alpha_filter(tok)]
BBC_list[:30]

#This is the tokenizer process that has been directly created to dealing with Twitter
#The example below is the standard text tokanizer, which is them run as the Twitter Tokanizer
tweet='Ukraine war: Russia accuses UK of provoking attacks on its territory'
ttokenizer = nltk.tokenize.TweetTokenizer()
tokens = ttokenizer.tokenize(tweet)
tokens

#This steps lowercases all of the words
#This is needed to look at the direct frequency of words that are not just "a","the", etc
all_tweet_tokens = [tok.lower() for tweet in BBC_l for tok in ttokenizer.tokenize(tweet)]
all_tweet_tokens[:30]

#The process was described with more precision above, but this filters out all of the non-aplhabetical words
tweet_token_list = [tok for tok in all_tweet_tokens if not alpha_filter(tok)]
tweet_token_list_wo_stop = [tok for tok in tweet_token_list if not tok in nltk_stopwords]

#This looks at the frequncy of  30 most popular words
BBCFD = nltk.FreqDist(tweet_token_list_wo_stop)
top_words = BBCFD.most_common(30)
for word, freq in top_words:
    print(word, freq)

#Question 2: How many favorites and retweets there are posted on each day of the week

#Creates a column that looks at the hour of the day

BBC_df['day']=BBC_df['created_at'].str[:3]

#Grouping data for these two variables for what day it was favorite and retweeted on average
BBC_d=BBC_df.groupby('day',as_index=False)['favorite_count', 'retweet_count'].mean()
#New names of the column
BBC_d.columns = [' Day of the Week', 'Favorites', 'Retweets']
BBC_d

#Question 3: Looking at who Posted more titles and have mentioned the war in Ukraine more, NY times or BBC news
#Opening up NY times twitter account
NY_timeline = api.user_timeline('nytimes', count=400)
type(timeline)

#Converting into list, and then json
NY_tweet_list = list(NY_timeline)
NY_tweet_json = [tweet._json for tweet in NY_tweet_list]

#Creating the pandas dataframe
NY_df = pd.DataFrame(NY_tweet_json)
NY_df

from datetime import datetime
from datetime import timedelta
#Looking for articles that were published over the past 3 days
three_days_ago = datetime.now() - timedelta(2)
three_days_ago

def convert_date(date):
    return datetime.strptime(date, '%a %b %d %H:%M:%S %z %Y')

#UTC offset in the form Â±HHMM[SS[.ffffff]] (empty string if the object is naive). (the variable z)

#Converting into the correct format
NY_df['created_at'] = NY_df['created_at'].apply(lambda x: convert_date(x))

BBC_df['created_at'] = BBC_df['created_at'].apply(lambda x: convert_date(x))

NY_df['created_at']

BBC_df['created_at']

#Looks at how many times Ukraine is mentioned in titles
NY_df[NY_df['text'].apply(lambda x: 'ukraine' in x.lower())]
UkraineTitle_df = NY_df[NY_df['text'].apply(lambda x: 'ukraine' in x.lower())]
print(len(UkraineTitle_df))
UkraineTitle_df.head()

#Looks at how many times is Ukraine War mentioned in NY times
target = 'ukraine'
targetu_df = NY_df[NY_df.apply(lambda x: mentions(x['text'], target) or mentions(x['text'], target), axis=1)]

print(len(NY_df))
target_df.head()

#BBC titles mentioning Ukraine
BBC_df[BBC_df['text'].apply(lambda x: 'ukraine' in x.lower())]
Ukraine1Title_df = BBC_df[BBC_df['text'].apply(lambda x: 'ukraine' in x.lower())]
print(len(Ukraine1Title_df))
Ukraine1Title_df.head()

#Looking at how many times BBC  mentioned it
target1 = 'urkaine'
target1_df = BBC_df[BBC_df.apply(lambda x: mentions(x['text'], target) or mentions(x['text'], target), axis=1)]

print(len(BBC_df))
target1_df.head()

#Since the number of the articles and titles mentioning Ukraine is nearly the same for both NY times and BBC news, the codes below look at what is the percentage of Ukraine war is out of the total coverage for both papers

#BBC news
print(target, 'as a topic, makes up, ',str(round(len(target1_df)*1.0/ len(BBC_df),2)*100),'% of the current news from BBC')

#NY times
print(target, 'as a topic, makes up, ',str(round(len(targetu_df)*1.0/ len(NY_df),2)*100),'% of the current news from BBC')

#Honestly, just for fun of it, because I found it super cool, I wanted to look at the Vader and Blob Sentiments of both news channels, and the results are interesting

#installing packages 
from nltk.sentiment.vader import SentimentIntensityAnalyzer

import nltk
nltk.download('vader_lexicon')
sid = SentimentIntensityAnalyzer()

from textblob import TextBlob

#Function that gives the sentiment score using the vader package
def vader_prediction(text):
  pred = sid.polarity_scores(text)
  mx = max([pred['neg'], pred['neu'], pred['pos']])
  if pred['neu'] == mx:
    return 0
  elif pred['neg'] == mx:
    return -1
  elif pred['pos'] == mx:
    return 1
  else:
    return 0

#Creates a function that gives the sentiment score using the textblob package
def blob_prediction(text):
  pred = TextBlob(text).sentiment.polarity
  if pred<0:
    return -1
  elif pred>0:
    return 1
  else:
    return 0

#Dataframe creation for BBC tweets
BBC_l = [doc['text'] for doc in BBC_json if 'text' in doc.keys()]
BBC1_df = pd.DataFrame(BBC_l, columns=['Tweet'])
BBC1_df["News Channel"]='BBC News'
BBC1_df

#Vader method sentiment for BBC nws
BBC1_df['vader_sentiment'] = BBC1_df['Tweet'].apply(lambda x: vader_prediction(x))
print('Mean Vader Sentiment Score for BBC News: ', BBC1_df['vader_sentiment'].mean())

#textblob method sentiment for BBC news
BBC1_df['blob_sentiment'] = BBC1_df['Tweet'].apply(lambda x: blob_prediction(x))
print('Mean Blob Sentiment Score for BBC News: ',BBC1_df['blob_sentiment'].mean())

#Dataframe for tweets from NY times
NY_l = [doc['text'] for doc in NY_tweet_json if 'text' in doc.keys()]
NY1_df = pd.DataFrame(NY_l, columns=['Tweet'])
NY1_df["News Channel"]='NY Times'
NY1_df

#Vader sentiment score for NY Times
NY1_df['vader_sentiment'] = NY1_df['Tweet'].apply(lambda x: vader_prediction(x))
print('Mean Vader Sentiment Score for NY Times: ',NY1_df['vader_sentiment'].mean())

#Textbolb sentiment score for NY Times
NY1_df['blob_sentiment'] = NY1_df['Tweet'].apply(lambda x: blob_prediction(x))
print('Mean Blob Sentiment Score for NY Times: ',NY1_df['blob_sentiment'].mean())

#Summary of the results
news_df = pd.concat([BBC1_df, NY1_df], axis=0)
news_df = news_df.groupby("News Channel", as_index=False)['vader_sentiment', 'blob_sentiment'].mean()
news_df.columns = [ 'News Channel','Vader Sentiment Score', 'Blob Sentiment Score']
news_df